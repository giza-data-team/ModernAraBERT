{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ModernAraBERT Quick Start Guide\n",
        "\n",
        "**5-minute tutorial to get started with ModernAraBERT**\n",
        "\n",
        "This notebook demonstrates:\n",
        "- Loading the model from Hugging Face\n",
        "- Basic inference for different tasks\n",
        "- Tokenization examples\n",
        "- Simple fine-tuning setup\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“¦ Installation\n",
        "\n",
        "First, ensure you have the required packages installed:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (uncomment if needed)\n",
        "# !pip install transformers torch datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸš€ Load ModernAraBERT\n",
        "\n",
        "Load the pre-trained ModernAraBERT model and tokenizer from Hugging Face:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Model name on Hugging Face\n",
        "MODEL_NAME = \"gizadatateam/ModernAraBERT\"\n",
        "\n",
        "# Load model and tokenizer\n",
        "print(f\"Loading {MODEL_NAME}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModel.from_pretrained(MODEL_NAME)\n",
        "\n",
        "print(f\"âœ… Model loaded successfully!\")\n",
        "print(f\"Vocabulary size: {len(tokenizer):,} tokens\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”¤ Tokenization Examples\n",
        "\n",
        "See how ModernAraBERT tokenizes Arabic text:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example Arabic sentences\n",
        "texts = [\n",
        "    \"Ù…Ø±Ø­Ø¨Ø§ Ø¨Ùƒ ÙÙŠ Ù†Ù…ÙˆØ°Ø¬ ModernAraBERT\",  # Welcome to ModernAraBERT model\n",
        "    \"Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…ÙØ¯Ø±Ø¨ Ø¹Ù„Ù‰ Ù†ØµÙˆØµ Ø¹Ø±Ø¨ÙŠØ© ÙƒØ¨ÙŠØ±Ø©\",  # This model is trained on large Arabic texts\n",
        "    \"ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙÙŠ Ù…Ù‡Ø§Ù… Ù…Ø®ØªÙ„ÙØ© Ù…Ø«Ù„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± ÙˆØ§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙŠØ§Ù†Ø§Øª\"  # Can be used for various tasks\n",
        "]\n",
        "\n",
        "print(\"ğŸ”¤ Tokenization Examples:\\\\n\")\n",
        "for i, text in enumerate(texts, 1):\n",
        "    # Tokenize\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    token_ids = tokenizer.encode(text, add_special_tokens=True)\n",
        "    \n",
        "    print(f\"Example {i}:\")\n",
        "    print(f\"  Text: {text}\")\n",
        "    print(f\"  Tokens: {tokens}\")\n",
        "    print(f\"  Token IDs: {token_ids}\")\n",
        "    print(f\"  Number of tokens: {len(tokens)}\\\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ§  Getting Embeddings\n",
        "\n",
        "Extract contextual embeddings from the model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample text\n",
        "text = \"Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ø­Ø¯ÙŠØ«\"  # Modern Arabic Language Model\n",
        "\n",
        "# Tokenize and convert to tensors\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "# Get embeddings\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state\n",
        "\n",
        "print(f\"Input text: {text}\")\n",
        "print(f\"\\\\nEmbedding shape: {embeddings.shape}\")\n",
        "print(f\"  - Batch size: {embeddings.shape[0]}\")\n",
        "print(f\"  - Sequence length: {embeddings.shape[1]}\")\n",
        "print(f\"  - Hidden dimension: {embeddings.shape[2]}\")\n",
        "\n",
        "# Get [CLS] token embedding (sentence representation)\n",
        "cls_embedding = embeddings[0, 0, :]\n",
        "print(f\"\\\\n[CLS] token embedding shape: {cls_embedding.shape}\")\n",
        "print(f\"[CLS] embedding (first 10 values): {cls_embedding[:10].tolist()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“Š Model Information\n",
        "\n",
        "Key facts about ModernAraBERT and next steps:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ğŸ“Š ModernAraBERT Model Information:\\\\n\")\n",
        "print(\"Base Model: ModernBERT (English)\")\n",
        "print(\"Extended Vocabulary: +80,000 Arabic tokens\")\n",
        "print(f\"Total Vocabulary Size: {len(tokenizer):,} tokens\")\n",
        "print(f\"\\\\nTraining Data:\")\n",
        "print(\"  - OSIAN corpus\")\n",
        "print(\"  - Arabic Billion Words\")\n",
        "print(\"  - Arabic Wikipedia\")\n",
        "print(\"  - OSCAR Arabic\")\n",
        "print(\"  - Total: ~17GB, 6M+ sentences\")\n",
        "print(\"\\\\nPretraining: 3 epochs (MLM objective)\")\n",
        "print(\"\\\\nBenchmark Results (vs AraBERT v1):\")\n",
        "print(\"  SA - HARD: 89.4% (+16.7%)\")\n",
        "print(\"  SA - AJGT: 70.5% (+12.5%)\")\n",
        "print(\"  NER - ANERCorp: 82.1% Micro-F1\")\n",
        "print(\"  QA - ARCD EM: 18.73% (+41.3%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”— Next Steps\n",
        "\n",
        "**Explore More:**\n",
        "- `02_pretraining_walkthrough.ipynb` - Learn how to pretrain from scratch\n",
        "- `03_benchmarking_examples.ipynb` - Run and visualize benchmarks\n",
        "\n",
        "**Documentation:**\n",
        "- [Pretraining Guide](../docs/PRETRAINING.md)\n",
        "- [Benchmarking Guide](../docs/BENCHMARKING.md)\n",
        "- [Model Card](../docs/MODEL_CARD.md)\n",
        "\n",
        "**Run Scripts:**\n",
        "```bash\n",
        "# Sentiment Analysis\n",
        "./scripts/benchmarking/run_sa_benchmark.sh gizadatateam/ModernAraBERT all ./results/sa\n",
        "\n",
        "# Named Entity Recognition  \n",
        "./scripts/benchmarking/run_ner_benchmark.sh gizadatateam/ModernAraBERT ./results/ner\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**Happy experimenting with ModernAraBERT! ğŸš€**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
