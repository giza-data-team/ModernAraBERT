{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ModernAraBERT Pretraining Walkthrough\n",
        "\n",
        "**Complete guide to pretraining ModernAraBERT from scratch**\n",
        "\n",
        "This notebook covers:\n",
        "1. Data collection and preprocessing\n",
        "2. Tokenizer vocabulary extension\n",
        "3. Model training with MLM objective\n",
        "4. Checkpointing and evaluation\n",
        "\n",
        "**Note**: This is a walkthrough tutorial. For production pretraining, use the provided scripts in `scripts/pretraining/`.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“‹ Prerequisites\n",
        "\n",
        "Ensure you have the required dependencies:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "# !pip install transformers torch datasets accelerate tokenizers farasa gdown rarfile PyYAML psutil\n",
        "\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Add repository root to path\n",
        "REPO_ROOT = Path.cwd().parent if 'notebooks' in str(Path.cwd()) else Path.cwd()\n",
        "sys.path.insert(0, str(REPO_ROOT))\n",
        "\n",
        "print(f\"Repository root: {REPO_ROOT}\")\n",
        "print(\"âœ… Environment setup complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“¥ Step 1: Data Collection\n",
        "\n",
        "Download pretraining datasets from the configured sources.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.pretraining.data_collection import download_and_extract_all_datasets\n",
        "\n",
        "# Configure paths\n",
        "links_json = REPO_ROOT / \"data\" / \"links.json\"\n",
        "raw_data_dir = REPO_ROOT / \"data\" / \"raw\"\n",
        "\n",
        "print(\"ğŸ“¥ Data Collection\")\n",
        "print(f\"Links file: {links_json}\")\n",
        "print(f\"Output directory: {raw_data_dir}\")\n",
        "print(\"\\\\nNote: This may take a while depending on your internet connection...\")\n",
        "print(\"For production, use: python scripts/pretraining/run_data_collection.py\")\n",
        "\n",
        "# Uncomment to actually download (can take hours):\n",
        "# download_and_extract_all_datasets(str(links_json), str(raw_data_dir))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”§ Step 2: Data Preprocessing\n",
        "\n",
        "Process the raw Arabic text data with:\n",
        "- Diacritics removal\n",
        "- Tatweel (elongation) removal\n",
        "- English word filtering\n",
        "- Word count filtering (100-8000 words per document)\n",
        "- Farasa morphological segmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.pretraining.data_preprocessing import (\n",
        "    normalize_arabic_text,\n",
        "    process_text_files_parallel,\n",
        "    segment_text_files_farasa,\n",
        "    split_data\n",
        ")\n",
        "\n",
        "# Example: Normalize Arabic text\n",
        "sample_text = \"Ø§Ù„Ø­ÙÙ…Ù’Ø¯Ù Ù„ÙÙ„ÙÙ‘Ù‡Ù Ø±ÙØ¨ÙÙ‘ Ø§Ù„Ù’Ø¹ÙØ§Ù„ÙÙ…ÙÙŠÙ€Ù€Ù€Ù€Ù€Ù†Ù\"  # With diacritics and tatweel\n",
        "normalized = normalize_arabic_text(sample_text)\n",
        "\n",
        "print(\"ğŸ”§ Text Normalization Example:\")\n",
        "print(f\"Original:   {sample_text}\")\n",
        "print(f\"Normalized: {normalized}\")\n",
        "print(\"\\\\nâœ… Preprocessing removes:\")\n",
        "print(\"  - Diacritics (Ø§Ù„Ø­ÙÙ…Ù’Ø¯Ù â†’ Ø§Ù„Ø­Ù…Ø¯)\")\n",
        "print(\"  - Tatweel/elongation (Ø§Ù„Ù’Ø¹ÙØ§Ù„ÙÙ…ÙÙŠÙ€Ù€Ù€Ù€Ù€Ù†Ù â†’ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠÙ†)\")\n",
        "print(\"  - English words\")\n",
        "print(\"  - Documents with <100 or >8000 words\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“š Step 3: Tokenizer Extension\n",
        "\n",
        "Extend ModernBERT's vocabulary with 80,000 Arabic-specific tokens learned from the preprocessed corpus.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Load base ModernBERT\n",
        "base_model_name = \"answerdotai/ModernBERT-base\"\n",
        "base_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "base_model = AutoModel.from_pretrained(base_model_name)\n",
        "\n",
        "print(\"ğŸ“š Tokenizer Extension\")\n",
        "print(f\"Base Model: {base_model_name}\")\n",
        "print(f\"Base Vocabulary Size: {len(base_tokenizer):,} tokens\")\n",
        "print(f\"Base Model Parameters: {sum(p.numel() for p in base_model.parameters()):,}\")\n",
        "print(\"\\\\nâœ… Extension Process:\")\n",
        "print(\"  1. Analyze vocabulary frequency in Arabic corpus\")\n",
        "print(\"  2. Select top 80,000 most frequent Arabic tokens\")\n",
        "print(\"  3. Add tokens to vocabulary (handling + segmentation markers)\")\n",
        "print(\"  4. Resize model embeddings to accommodate new tokens\")\n",
        "print(\"  5. New vocabulary size: ~230,000 tokens\")\n",
        "print(\"\\\\nFor production, use: python scripts/pretraining/run_tokenizer_extension.py\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ Step 4: Pretraining with MLM\n",
        "\n",
        "Train the extended model using Masked Language Modeling (MLM) objective.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ğŸ¯ MLM Pretraining Configuration\")\n",
        "print(\"\\\\nTraining Setup:\")\n",
        "print(\"  - Objective: Masked Language Modeling (15% masking)\")\n",
        "print(\"  - Epochs: 3 (2 @ 128 tokens, 1 @ 512 tokens)\")\n",
        "print(\"  - Optimizer: AdamW (lr=5e-5)\")\n",
        "print(\"  - Scheduler: Cosine with warmup\")\n",
        "print(\"  - Batch Size: 32 per device\")\n",
        "print(\"  - Gradient Accumulation: 4 steps\")\n",
        "print(\"  - Mixed Precision: FP16\")\n",
        "print(\"  - Hardware: NVIDIA A100 40GB\")\n",
        "print(\"\\\\nAdvanced Features:\")\n",
        "print(\"  âœ… Distributed training with Accelerate\")\n",
        "print(\"  âœ… Automatic checkpointing\")\n",
        "print(\"  âœ… Memory profiling\")\n",
        "print(\"  âœ… torch.compile optimization\")\n",
        "print(\"\\\\nFor production training:\")\n",
        "print(\"  python scripts/pretraining/run_pretraining.py --config configs/pretraining_config.yaml\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ“Š Step 5: Monitoring and Evaluation\n",
        "\n",
        "Track training progress and evaluate the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"ğŸ“Š Training Monitoring\")\n",
        "print(\"\\\\nMetrics Tracked:\")\n",
        "print(\"  - MLM Loss (training and validation)\")\n",
        "print(\"  - MLM Accuracy (% of correct predictions)\")\n",
        "print(\"  - Perplexity\")\n",
        "print(\"  - Learning rate schedule\")\n",
        "print(\"  - Memory usage (RAM + VRAM)\")\n",
        "print(\"  - Training throughput (samples/second)\")\n",
        "print(\"\\\\nCheckpointing:\")\n",
        "print(\"  - Auto-save every 1000 steps\")\n",
        "print(\"  - Keep best 3 checkpoints\")\n",
        "print(\"  - Resume from any checkpoint\")\n",
        "print(\"\\\\nExpected Results (3 epochs on ~17GB data):\")\n",
        "print(\"  - Training time: ~24-48 hours on A100\")\n",
        "print(\"  - Final MLM loss: ~2.5-3.0\")\n",
        "print(\"  - Final perplexity: ~12-20\")\n",
        "print(\"  - Memory usage: ~35GB VRAM\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸš€ Production Workflow\n",
        "\n",
        "Complete command sequence for pretraining:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```bash\n",
        "# 1. Download datasets\n",
        "python scripts/pretraining/run_data_collection.py \\\\\n",
        "    --links-json data/links.json \\\\\n",
        "    --output-dir data/raw\n",
        "\n",
        "# 2. Preprocess data (full pipeline)\n",
        "python scripts/pretraining/run_data_preprocessing.py \\\\\n",
        "    --input-dir data/raw \\\\\n",
        "    --output-dir data/processed \\\\\n",
        "    --all\n",
        "\n",
        "# 3. Extend tokenizer\n",
        "python scripts/pretraining/run_tokenizer_extension.py \\\\\n",
        "    --model-name answerdotai/ModernBERT-base \\\\\n",
        "    --input-dir data/processed/segmented \\\\\n",
        "    --output-dir models/modernarabert_extended \\\\\n",
        "    --max-vocab-size 80000\n",
        "\n",
        "# 4. Run pretraining (single GPU)\n",
        "python scripts/pretraining/run_pretraining.py \\\\\n",
        "    --config configs/pretraining_config.yaml\n",
        "\n",
        "# 4b. Run pretraining (multi-GPU with Accelerate)\n",
        "accelerate launch scripts/pretraining/run_pretraining.py \\\\\n",
        "    --config configs/pretraining_config.yaml\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“– Additional Resources\n",
        "\n",
        "- **Detailed Guide**: [docs/PRETRAINING.md](../docs/PRETRAINING.md)\n",
        "- **Configuration**: [configs/pretraining_config.yaml](../configs/pretraining_config.yaml)\n",
        "- **Source Code**: [src/pretraining/](../src/pretraining/)\n",
        "\n",
        "---\n",
        "\n",
        "**Next**: Check out `03_benchmarking_examples.ipynb` to evaluate your trained model!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
